{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mgyy916kkxgeoqaqru8ph",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DEBUG = False\n",
    "import warnings\n",
    "if not DEBUG:\n",
    "    warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "ixw57f65kgq9qbj0a97jdl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "YADSPH = False\n",
    "if YADSPH:\n",
    "    %pip install tensorflow==2.3.1\n",
    "    %pip install tensorflow-addons==0.11.2\n",
    "    %pip install efficientnet\n",
    "    %pip install imgaug\n",
    "    %pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "3r4iewok5xl0rfq30p09pfb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "import warnings\n",
    "if not DEBUG:\n",
    "    warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as A\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Model, Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras.metrics import AUC, CategoricalAccuracy\n",
    "from tqdm import tqdm\n",
    "import efficientnet.tfkeras as efn\n",
    "print('tensorflow version:', tf.__version__)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpu_devices:\n",
    "    for gpu_device in gpu_devices:\n",
    "        print('device available:', gpu_device)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "f839um3z8luhc7xusgu1wr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TWOCLS = False\n",
    "VER = 'vbin1' if TWOCLS else 'v14'\n",
    "PARAMS = {\n",
    "    'version': VER,\n",
    "    'folds': 5,\n",
    "    'folds_train': 2 if DEBUG else None,\n",
    "    'img_size': 512, # 224=B0 240=B1 260=B2 300=B3 380=B4 456=B5 528=B6 600=B7\n",
    "    'epochs': 4 if DEBUG else 100,\n",
    "    'patience': 2 if DEBUG else 10,\n",
    "    'decay': False,\n",
    "    'batch_size': 4,\n",
    "    'backbone': 3, # efficientnetbX => X from 0 to 7\n",
    "    'seed': 2021,\n",
    "    'aughard': False if DEBUG else True,\n",
    "    'lr': .0005,\n",
    "    'lbl_smth': .0001,\n",
    "    'metric': 'auc', # 'categorical_accuracy'\n",
    "    'pseudo_th': None,\n",
    "    'comments': ''\n",
    "}\n",
    "DATA_PATH = './data'\n",
    "if YADSPH:\n",
    "    DATASET_PATH = f'./data2/SIIM-COVID19-Resized/img_sz_{PARAMS[\"img_size\"]}'\n",
    "    IMGS_PATH = f'{DATASET_PATH}/train'\n",
    "else:\n",
    "    if PARAMS['pseudo_th']:\n",
    "        IMGS_PATH = f'{DATA_PATH}/train_{PARAMS[\"img_size\"]}_psd'\n",
    "    else:\n",
    "        IMGS_PATH = f'{DATA_PATH}/train_{PARAMS[\"img_size\"]}'\n",
    "MDLS_PATH = f'./models_{VER}'\n",
    "if not os.path.exists(MDLS_PATH):\n",
    "    os.mkdir(MDLS_PATH)\n",
    "with open(f'{MDLS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(PARAMS, file)\n",
    "    \n",
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "seed_all(PARAMS['seed'])\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "9v55l3ulg5t3ua1ryq4ml",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if YADSPH:\n",
    "    train_df = pd.read_csv(f'{DATASET_PATH}/meta_sz_{PARAMS[\"img_size\"]}.csv')\n",
    "else:\n",
    "    train_df = pd.read_csv(f'{DATA_PATH}/train_meta_{PARAMS[\"img_size\"]}.csv')\n",
    "display(train_df.head())\n",
    "if DEBUG:\n",
    "    train_df = train_df.sample(100)\n",
    "df_train_img = pd.read_csv(f'{DATA_PATH}/train_image_level.csv')\n",
    "df_train_sty = pd.read_csv(f'{DATA_PATH}/train_study_level.csv')\n",
    "\n",
    "if YADSPH:\n",
    "    train_df['img'] = train_df['image_id'].apply(lambda x: ''.join([x, '.jpg']))\n",
    "    train_df.rename(columns={'dim1': 'dim_x', 'dim0': 'dim_y'}, inplace=True)\n",
    "    train_df['id'] = train_df['img'].apply(lambda x: x.split('/')[-1].replace('.jpg', '_image'))\n",
    "else:\n",
    "    train_df['id'] = train_df['img'].apply(lambda x: x.split('/')[-1].replace('.png', '_image'))\n",
    "\n",
    "df_train_sty['StudyInstanceUID'] = df_train_sty['id'].apply(lambda x: x.replace('_study', ''))\n",
    "del df_train_sty['id']\n",
    "df_train_img = df_train_img.merge(df_train_sty, on='StudyInstanceUID')\n",
    "train_df = df_train_img.merge(train_df, on='id')\n",
    "train_df['None Opacity'] = train_df['boxes'].isnull()\n",
    "print(train_df.shape)\n",
    "display(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if PARAMS['pseudo_th']:\n",
    "    df_pseudo_img = pd.read_csv(f'{DATA_PATH}/pseudo_study_level.csv')\n",
    "    df_pseudo_img['StudyInstanceUID'] = df_pseudo_img['id'].apply(lambda x: x.replace('_study', ''))\n",
    "    df_pseudo_img['id'] = df_pseudo_img['img']\n",
    "    df_pseudo_img['img'] = df_pseudo_img['img'].apply(lambda x: x.replace('_image', '.png'))\n",
    "    cols = [\n",
    "        'Negative for Pneumonia',\n",
    "        'Typical Appearance',\n",
    "        'Indeterminate Appearance',\n",
    "        'Atypical Appearance'\n",
    "    ]\n",
    "    df_pseudo_img[cols] = np.where(df_pseudo_img[cols] > PARAMS['pseudo_th'], 1, 0)\n",
    "    df_pseudo_img['None Opacity'] = df_pseudo_img['None Opacity'].apply(\n",
    "        lambda x: True if x > PARAMS['pseudo_th'] else False\n",
    "    )\n",
    "    df_pseudo_img = df_pseudo_img.drop(df_pseudo_img[df_pseudo_img[cols].sum(axis=1) == 0].index)\n",
    "    print(df_pseudo_img.shape)\n",
    "    display(df_pseudo_img.head())\n",
    "    train_df = train_df.append(df_pseudo_img)\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "wa98sep78ym7z22m5z4l54",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bar_plot(train_df, variable):\n",
    "    var = train_df[variable]\n",
    "    varValue = var.value_counts()\n",
    "    plt.figure(figsize = (12, 3))\n",
    "    plt.bar(varValue.index, varValue)\n",
    "    plt.xticks(varValue.index, varValue.index.values)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(variable)\n",
    "    plt.show()\n",
    "    print(\"{}: \\n{}\".format(variable, varValue))\n",
    "\n",
    "train_df['target'] = 'Negative for Pneumonia'\n",
    "train_df.loc[train_df['Typical Appearance'] == 1, 'target'] = 'Typical Appearance'\n",
    "train_df.loc[train_df['Indeterminate Appearance'] == 1, 'target'] = 'Indeterminate Appearance'\n",
    "train_df.loc[train_df['Atypical Appearance'] == 1, 'target'] = 'Atypical Appearance'\n",
    "bar_plot(train_df, 'target') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "733sa4nrdl0s7afph32bfc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True)\n",
    "classes = [\n",
    "    'Negative for Pneumonia',\n",
    "    'Typical Appearance', \n",
    "    'Indeterminate Appearance', \n",
    "    'Atypical Appearance'\n",
    "]\n",
    "print('classes:\\n', classes,\n",
    "      '\\nclasses labels:\\n', np.unique(train_df[classes].values, axis=0))\n",
    "label2classes = {\n",
    "    '[1, 0, 0, 0]': classes[0],\n",
    "    '[0, 1, 0, 0]': classes[1],\n",
    "    '[0, 0, 1, 0]': classes[2],\n",
    "    '[0, 0, 0, 1]': classes[3]\n",
    "}\n",
    "PARAMS['classes'] = classes\n",
    "with open(f'{MDLS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(PARAMS, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "py75nniflc9u0fnwkqiy7"
   },
   "source": [
    "# Data generator and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "nmsihrqpuin630ehn3gjzb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "class DataGenSIIM(Sequence):\n",
    "    \n",
    "    def __init__(self, df, classes, imgs_path, imgs_idxs, img_size,\n",
    "                 batch_size=8, mode='fit', shuffle=False, aug=None, \n",
    "                 resize=None, two_cls=False):\n",
    "        self.df = df\n",
    "        self.classes = classes\n",
    "        self.imgs_path = imgs_path\n",
    "        self.imgs_idxs = imgs_idxs\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.mode = mode\n",
    "        self.shuffle = shuffle\n",
    "        self.aug = aug\n",
    "        self.resize = resize\n",
    "        self.two_cls = two_cls\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.imgs_idxs) / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.imgs_idxs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        batch_size = min(self.batch_size, len(self.imgs_idxs) - index*self.batch_size)\n",
    "        X = np.zeros((batch_size, self.img_size, self.img_size, 3), dtype=np.float32)\n",
    "        imgs_batch = self.imgs_idxs[index * self.batch_size : (index+1) * self.batch_size]\n",
    "        if self.mode == 'fit':\n",
    "            if self.two_cls:\n",
    "                y = np.zeros(batch_size, dtype=np.float32)\n",
    "            else:\n",
    "                y = np.zeros((batch_size, len(self.classes)), dtype=np.float32)\n",
    "            for i, img_idx in enumerate(imgs_batch):\n",
    "                X[i, ], y[i] = self.get_img(img_idx)\n",
    "            return X, y\n",
    "        elif self.mode == 'predict':\n",
    "            for i, img_idx in enumerate(imgs_batch):\n",
    "                X[i, ] = self.get_img(img_idx)\n",
    "            return X\n",
    "        else:\n",
    "            raise AttributeError('fit mode parameter error')\n",
    "            \n",
    "    def get_img(self, img_idx):\n",
    "        img_path = f'{self.imgs_path}/{img_idx}'\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print('error load image:', img_path)\n",
    "        if self.resize:\n",
    "            img = cv2.resize(img, (int(img.shape[1] / self.resize), int(img.shape[0] / self.resize)))\n",
    "        img = img.astype(np.float32) / 255\n",
    "        if self.mode == 'fit':\n",
    "            if self.two_cls:\n",
    "                label = self.df.loc[self.df['img'] == img_idx, 'None Opacity'].values[0]\n",
    "            else:\n",
    "                label = self.df.loc[self.df['img'] == img_idx, self.classes].values[0]\n",
    "            if label is None:\n",
    "                print('error load label:', img_path)\n",
    "            label = label.astype(np.float32)\n",
    "            if self.aug:\n",
    "                img = self.aug(image=img)['image']\n",
    "            return img, label\n",
    "        else:\n",
    "            if self.aug:\n",
    "                img = self.aug(image=img)['image']\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "5c6y2b505s6zmyoxnp5x1a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "if PARAMS['aughard']:\n",
    "    aug = A.Compose([\n",
    "        A.OneOf([\n",
    "            A.RandomBrightness(limit=.2, p=1), \n",
    "            A.RandomContrast(limit=.2, p=1), \n",
    "            A.RandomGamma(p=1)\n",
    "        ], p=.5),\n",
    "        A.OneOf([\n",
    "            A.Blur(blur_limit=3, p=1),\n",
    "            A.MedianBlur(blur_limit=3, p=1)\n",
    "        ], p=.25),\n",
    "        A.OneOf([\n",
    "            A.GaussNoise(0.002, p=.5),\n",
    "            A.augmentations.geometric.transforms.Affine(p=.5) if YADSPH else A.IAAAffine(p=.5),\n",
    "        ], p=.25),\n",
    "        A.OneOf([\n",
    "            A.ElasticTransform(alpha=120, sigma=120 * .05, alpha_affine=120 * .03, p=.5),\n",
    "            A.GridDistortion(p=.5),\n",
    "            A.OpticalDistortion(distort_limit=2, shift_limit=.5, p=1)                  \n",
    "        ], p=.25),\n",
    "        A.RandomRotate90(p=.5),\n",
    "        A.HorizontalFlip(p=.5),\n",
    "        A.VerticalFlip(p=.5),\n",
    "        A.Cutout(num_holes=10, \n",
    "                 max_h_size=int(.1 * PARAMS['img_size']), max_w_size=int(.1 * PARAMS['img_size']), \n",
    "                 p=.25),\n",
    "        A.ShiftScaleRotate(p=.5)\n",
    "    ])\n",
    "else:\n",
    "    aug = A.Compose([\n",
    "            A.OneOf([\n",
    "                A.RandomBrightness(limit=.2, p=1), \n",
    "                A.RandomContrast(limit=.2, p=1), \n",
    "                A.RandomGamma(p=1)\n",
    "            ], p=.5),\n",
    "            A.HorizontalFlip(p=.5),\n",
    "            A.ShiftScaleRotate(p=.25, rotate_limit=0)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "0vjja3th7dpi574l3qymp05",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "imgs_idxs = train_df.img.values\n",
    "test_datagen = DataGenSIIM(\n",
    "    df=train_df,\n",
    "    classes=classes,\n",
    "    imgs_path=IMGS_PATH, \n",
    "    imgs_idxs=imgs_idxs, \n",
    "    img_size=PARAMS['img_size'], \n",
    "    batch_size=PARAMS['batch_size'], \n",
    "    mode='fit', \n",
    "    shuffle=True,           \n",
    "    aug=aug, \n",
    "    resize=None,\n",
    "    two_cls=TWOCLS\n",
    ")\n",
    "bsize = min(4, PARAMS['batch_size'])\n",
    "Xt, yt = test_datagen.__getitem__(0)\n",
    "print('test X: ', Xt.shape)\n",
    "print('test y: ', yt.shape)\n",
    "fig, axes = plt.subplots(figsize=(16, 4), nrows=1, ncols=bsize)\n",
    "for j in range(bsize):\n",
    "    title = yt[j] if TWOCLS else classes[np.argmax(yt[j])]\n",
    "    axes[j].imshow(Xt[j])\n",
    "    axes[j].set_title(title)\n",
    "    axes[j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "iz5nbga9czi5xrhrr4gtpl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, \n",
    "        efn.EfficientNetB2, efn.EfficientNetB3, \n",
    "        efn.EfficientNetB4, efn.EfficientNetB5, \n",
    "        efn.EfficientNetB6, efn.EfficientNetB7]\n",
    "\n",
    "def get_model(params, classes=4, lr=.001, lbl_smth=.0001):\n",
    "    input_shape=(params['img_size'], params['img_size'], 3)\n",
    "    enet = EFNS[params['backbone']](\n",
    "        input_shape=input_shape,\n",
    "        weights='imagenet',\n",
    "        include_top=False\n",
    "    )\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = enet(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(64, activation = 'relu')(x)\n",
    "    if classes == 1:\n",
    "        x = Dense(classes, activation='sigmoid')(x)\n",
    "        loss = BinaryCrossentropy(label_smoothing=params['lbl_smth'])\n",
    "        auc = tf.keras.metrics.AUC(name='auc')\n",
    "        accuracy = 'accuracy'\n",
    "        f1  = tfa.metrics.F1Score(\n",
    "            num_classes=classes, \n",
    "            average='macro', \n",
    "            threshold=None\n",
    "        )\n",
    "    else:\n",
    "        x = Dense(classes, activation='softmax')(x)\n",
    "        loss = CategoricalCrossentropy(label_smoothing=params['lbl_smth'])\n",
    "        auc = AUC(name='auc', curve='ROC', multi_label=True)\n",
    "        accuracy = CategoricalAccuracy()\n",
    "        f1  = tfa.metrics.F1Score(\n",
    "            num_classes=classes, \n",
    "            average='macro', \n",
    "            threshold=None\n",
    "        )\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(\n",
    "        optimizer=tfa.optimizers.Lookahead(\n",
    "            tf.keras.optimizers.Adam(learning_rate=params['lr']),\n",
    "            sync_period=max(6, int(params['patience'] / 4))\n",
    "        ),\n",
    "        loss=loss, \n",
    "        metrics=[auc, accuracy, f1]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "a10aoha8c6ho95ggluitj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def get_lr_callback(batch_size=10, epochs=100, warmup=5, plot=False):\n",
    "    lr_start = 1e-5\n",
    "    lr_max = 1e-3\n",
    "    lr_min = lr_start / 100\n",
    "    lr_ramp_ep = warmup\n",
    "    lr_sus_ep = 0\n",
    "    lr_decay = .95\n",
    "    \n",
    "    def lr_scheduler(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay ** (epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        return lr\n",
    "        \n",
    "    if not plot:\n",
    "        lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=False)\n",
    "        return lr_callback \n",
    "    else: \n",
    "        return lr_scheduler\n",
    "    \n",
    "if PARAMS['decay']:\n",
    "    lr_scheduler_plot = get_lr_callback(\n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        epochs=PARAMS['epochs'], \n",
    "        plot=True\n",
    "    )\n",
    "    xs = [i for i in range(PARAMS['epochs'])]\n",
    "    y = [lr_scheduler_plot(x) for x in xs]\n",
    "    plt.plot(xs, y)\n",
    "    plt.title(f'lr schedule from {y[0]:.5f} to {max(y):.3f} to {y[-1]:.8f}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "mqz4dmgs1mdijk6pc06ad",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "def train_model(mparams, n_fold, train_datagen, val_datagen):\n",
    "    model = get_model(\n",
    "        mparams,\n",
    "        classes=1 if TWOCLS else 4\n",
    "    )\n",
    "    checkpoint_path = f'{MDLS_PATH}/model_{n_fold}.hdf5'\n",
    "    earlystopper = EarlyStopping(\n",
    "        monitor=f'val_{mparams[\"metric\"]}', \n",
    "        patience=mparams['patience'], \n",
    "        verbose=0,\n",
    "        restore_best_weights=True,\n",
    "        mode='max'\n",
    "    )\n",
    "    lrreducer = ReduceLROnPlateau(\n",
    "        monitor=f'val_{mparams[\"metric\"]}', \n",
    "        factor=.1, \n",
    "        patience=int(mparams['patience'] / 2), \n",
    "        verbose=0, \n",
    "        min_lr=1e-7,\n",
    "        mode='max'\n",
    "    )\n",
    "    checkpointer = ModelCheckpoint(\n",
    "        checkpoint_path, \n",
    "        monitor=f'val_{mparams[\"metric\"]}', \n",
    "        verbose=0, \n",
    "        save_best_only=True,\n",
    "        save_weights_only=True, \n",
    "        mode='max'\n",
    "    )\n",
    "    callbacks = [earlystopper, checkpointer]\n",
    "    if mparams['decay']:\n",
    "        callbacks.append(get_lr_callback(mparams['batch_size']))\n",
    "        print('lr warmup and decay')\n",
    "    else:\n",
    "        callbacks.append(lrreducer)\n",
    "        print('lr reduce on plateau')\n",
    "    history = model.fit(\n",
    "        train_datagen,\n",
    "        validation_data=val_datagen,\n",
    "        callbacks=callbacks,\n",
    "        epochs=mparams['epochs'],\n",
    "        verbose=1\n",
    "    )\n",
    "    history_file = f'{MDLS_PATH}/history_{n_fold}.json'\n",
    "    dict_to_save = {}\n",
    "    for k, v in history.history.items():\n",
    "        dict_to_save.update({k: [np.format_float_positional(x) for x in history.history[k]]})\n",
    "    with open(history_file, 'w') as file:\n",
    "        json.dump(dict_to_save, file)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "bd0m5tgh1gid24kpkr46bf"
   },
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "u2t5y07rnjr4jvlodo3je2",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "skf  = StratifiedKFold(n_splits=PARAMS['folds'])\n",
    "train_df['fold'] = -1\n",
    "if TWOCLS:\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y=train_df['None Opacity'])):\n",
    "        train_df.loc[val_idx, 'fold'] = fold    \n",
    "else:\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, y=train_df.target)):\n",
    "        train_df.loc[val_idx, 'fold'] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "rcd8bkqyx8saak7zsl0fnk",
    "execution_id": "47341ce9-0db2-4485-af3f-e90fc578e8da",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "epoch_by_folds = []\n",
    "loss_by_folds = []\n",
    "metric_by_folds = []\n",
    "\n",
    "if DEBUG:\n",
    "    n_folds_train = 2\n",
    "else:\n",
    "    n_folds_train = PARAMS['folds'] if not PARAMS['folds_train'] else PARAMS['folds_train']\n",
    "start_folds_train = 0\n",
    "\n",
    "for fold_num in range(start_folds_train, n_folds_train):\n",
    "    print('=' * 20, 'FOLD:', fold_num, '=' * 20)\n",
    "    target = 'None Opacity' if TWOCLS else 'target'\n",
    "    train_idxs = train_df.loc[train_df['fold'] != fold_num, 'img'].values\n",
    "    print('-' * 30, f'\\nTRAIN STATS: {len(train_idxs)}\\n', \n",
    "          train_df.loc[train_df['fold'] != fold_num, target].value_counts())\n",
    "    val_idxs = train_df.loc[train_df['fold'] == fold_num, 'img'].values\n",
    "    print('-' * 30, f'\\nVAL STATS: {len(val_idxs)}\\n',\n",
    "          train_df.loc[train_df['fold'] == fold_num, target].value_counts(),\n",
    "          '\\n', '-' * 30)\n",
    "    train_datagen = DataGenSIIM(\n",
    "        df=train_df,\n",
    "        classes=classes,\n",
    "        imgs_path=IMGS_PATH, \n",
    "        imgs_idxs=train_idxs, \n",
    "        img_size=PARAMS['img_size'], \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        mode='fit', \n",
    "        shuffle=True,           \n",
    "        aug=aug, \n",
    "        resize=None,\n",
    "        two_cls=TWOCLS\n",
    "    )\n",
    "    val_datagen = DataGenSIIM(\n",
    "        df=train_df,\n",
    "        classes=classes,\n",
    "        imgs_path=IMGS_PATH, \n",
    "        imgs_idxs=val_idxs, \n",
    "        img_size=PARAMS['img_size'], \n",
    "        batch_size=PARAMS['batch_size'], \n",
    "        mode='fit', \n",
    "        shuffle=False,           \n",
    "        aug=None, \n",
    "        resize=None,\n",
    "        two_cls=TWOCLS\n",
    "    )\n",
    "    model, history = train_model(PARAMS, fold_num, train_datagen, val_datagen)\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.plot(history.history[f'{PARAMS[\"metric\"]}'], label=f'{PARAMS[\"metric\"]}')\n",
    "    plt.plot(history.history[f'val_{PARAMS[\"metric\"]}'], label=f'val {PARAMS[\"metric\"]}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    best_epoch = np.argmax(history.history[f'val_{PARAMS[\"metric\"]}'])\n",
    "    best_loss = history.history['val_loss'][best_epoch]\n",
    "    best_metric = history.history[f'val_{PARAMS[\"metric\"]}'][best_epoch]\n",
    "    print('best epoch:', best_epoch, \n",
    "          '| best loss:', best_loss,\n",
    "          f'| best {PARAMS[\"metric\"]}:', best_metric)\n",
    "    epoch_by_folds.append(best_epoch)\n",
    "    loss_by_folds.append(best_loss)\n",
    "    metric_by_folds.append(best_metric)\n",
    "    del train_datagen, val_datagen, model; gc.collect()\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(f'time elapsed: {elapsed_time // 60:.0f} min {elapsed_time % 60:.0f} sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "gp0gpryo5g5mavp4aailrh",
    "execution_id": "1aadbe9b-5762-4e2a-b690-24120ecae4a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = PARAMS.copy()\n",
    "result['bavg_epoch'] = np.mean(epoch_by_folds)\n",
    "result['bavg_loss'] = np.mean(loss_by_folds)\n",
    "result[f'bavg_{PARAMS[\"metric\"]}'] = np.mean(metric_by_folds)\n",
    "result[f'{PARAMS[\"metric\"]}_by_folds'] = ' '.join([f'{x:.4f}' for x in metric_by_folds])\n",
    "result['classes'] = ', '.join(classes)\n",
    "with open(f'{MDLS_PATH}/params.json', 'w') as file:\n",
    "    json.dump(result, file)\n",
    "if not os.path.exists('results.csv'):\n",
    "    df_save = pd.DataFrame(result, index=[0])\n",
    "    df_save.to_csv('results.csv', sep='\\t')\n",
    "else:\n",
    "    df_old = pd.read_csv('results.csv', sep='\\t', index_col=0)\n",
    "    df_save = pd.DataFrame(result, index=[df_old.index.max() + 1])\n",
    "    df_save = df_old.append(df_save, ignore_index=True)\n",
    "    df_save.to_csv('results.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "fwfightswj8z858cea8pw",
    "execution_id": "c42bc1f0-998a-4b03-af75-0e2e5fbd6f12",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv('results.csv', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellId": "kfvzfz2ublci5msrjxob",
    "execution_id": "73f6177e-c2ac-4f8f-8dc6-1d4613ff82e1",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Orange Python 3",
   "language": "python",
   "name": "orange"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "notebookId": "fd0fddcf-8709-4246-ab11-d26be103cc9d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
